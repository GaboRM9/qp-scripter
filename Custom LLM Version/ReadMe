QxScripter - CustomLLM version

QxScripter is a custom model specifically trained to support QuestionPro when creating custom made scripts for JS logics.

Getting Started:

On this time we are going to use the following tools to create a customLLM for this purpose:

Prerequisites
1 - Python 3.10+.
2 - Ollama to run local models - https://ollama.com/.
3 - Model: llamaCode - Meta's open-source code interpreter model. - #You can download this model here: https://huggingface.co/TheBloke/Llama-2-Coder-7B-GGUF, you can download >7b parameter models but if your computer has less than 8GB RAM expect crashes or a veeery slow performance. 
4 - OpenAI or any Open source embeding conversor.
5 - Repository containing knowledge files (Im using the Knowledge folder to store this files). *Its very important to standarize and format the data the best way possible before generate the embeddings.
6 - LangChain - Preprocessing and data transformation.
7 - Vector DB - On langchain documentation they use Chroma, but if you need other one you could adapt the connector.

Installation:

Clone this repo and create a folder called "model" inside the main folder (Custom LLM Version) then place the downloaded model inside, this model should be accesible from relative path here:

        model_path="Custom LLM Version/model/codellama-7b.Q5_K_S.gguf" 

        By default we are going to ignore this folder when doing pushs/PR. This until we have somewhere to host the model we want.

Install the initial dependencies. Open your terminal and run the following command:

        pip3 install --upgrade --quiet langchain-openai tiktoken chromadb langchain

        pip3 install llama-cpp-python #if needed

        If you experience any bottle neck try runing a previous version of llama-cpp-python:

        pip3 uninstall llama-cpp-python
        CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python==0.2.27

        This will get faster responses in exchange of RAM.

This command will install all the necessary libraries, including LangChain, OpenAI, TikToken, and ChromaDB, ensuring you have the latest versions.

Running the Script
After installing the dependencies and ensuring all files are correctly placed, you can run the QxScripter.py script. This script is the heart of our project and leverages the functionalities provided by the installed libraries.

Workflow Overview
The project workflow is based on the documentation provided at LangChain Code Understanding:

https://python.langchain.com/docs/use_cases/code_understanding

Here's a brief overview:

1. Update Data to Repository
Objective: Maintain a dedicated GitHub repository (or a bucket) to store clean data that QxScripter.py will use.
Requirements: The repository should only contain relevant files, properly named and with the correct extension based on the programming language in use.
2. Document Pre-Processing
Using LangChain: We employ LangChain to read, understand, and split documents, preparing them for further processing.
3. Generate and Store Embeddings
Embedding Generation: Utilize the OpenAI API to generate embeddings from the pre-processed data.
Database Integration: ChromaDB will then create a database object to store these embeddings, facilitating efficient retrieval and analysis.
4. Retrieval and Analysis (RAG)

Before the query we need to format the prompt, example script for llama code

def prompt_format():
    SYSTEM_PROMPT = B_SYS + system_prompt + E_SYS
    prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
    return prompt_template

Retriever Functionality: The system retrieves relevant information from the database based on user queries, identifying similarities and patterns.

5. Testing and Deployment
Initial Testing: For initial tests, queries are hardcoded into the script. However, you can modify this to suit your testing needs.

        # Retrieve documents for a question only if the retriever is initialized
        if 'retriever' in locals():
            question = "How can I hide the Q2 if var <1?" #Hard coded query
            docs = retriever.get_relevant_documents(question)


Deployment: Successfully running the script to this stage indicates a local deployment of your custom LLM code interpreter. Congrats!

Next Steps
While achieving local deployment is a significant milestone, our journey doesn't stop here. The current model requires fine-tuning to enhance its speed and consistency in providing answers. This fine-tuning process is crucial for improving the model's performance and utility.

Contribution
Your contributions are invaluable to the growth and improvement of this project. Whether it's through suggesting new features, improving the code, or helping with model fine-tuning, every bit helps. Let's collaborate to make this tool more powerful and accessible to everyone.
